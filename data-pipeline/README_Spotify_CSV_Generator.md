# Spotify CSV Generator (10k tracks per run)

## What it does

Collects ~10,000 unique Spotify tracks per run via the Spotify Web API (Spotipy).

Outputs timestamped CSVs in `exports/` with key fields: title, artists, genres, etc.

Deduplicates across runs using `seen_track_ids.jsonl` so every file contains new songs.

Users can run the script repeatedly (e.g., 100 times) to reach ~1,000,000 tracks.

## Prerequisites

- A Spotify Client ID and Client Secret (from [developer.spotify.com](https://developer.spotify.com/dashboard))
- Python 3.10+ (3.11 recommended). Works great in Conda or venv.

## Setup

```bash
# clone repo, cd into it
# create an environment (choose one)

# Option A: Conda
conda create -n spotify python=3.11 -y
conda activate spotify

# Option B: venv
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# install deps
python -m pip install --upgrade pip setuptools wheel
python -m pip install spotipy python-dotenv pandas
```

Create a `.env` file (do NOT commit real secrets):

```env
SPOTIFY_CLIENT_ID=your_id_here
SPOTIFY_CLIENT_SECRET=your_secret_here
```

## Generate one CSV (10k tracks)

```bash
TARGET_TRACKS=10000 python fetch_spotify_10k_min.py
```

You should see logs like:

```
[using] OUTPUT_DIR=.../exports
[using] SEEN_FILE=.../seen_track_ids.jsonl
[done] wrote 10000 tracks -> exports/tracks_10000_YYYYMMDDTHHMMSSZ.csv
[done] appended 10000 new IDs to seen_track_ids.jsonl
```

## Make many CSVs (batch runs)

Paste the command multiple times, or use a loop:

```bash
# 10 files (~100k tracks)
for i in {1..10}; do TARGET_TRACKS=10000 python fetch_spotify_10k_min.py; done

# 100 files (~1M tracks)
for i in {1..100}; do TARGET_TRACKS=10000 python fetch_spotify_10k_min.py; done
```

**Windows PowerShell:**
```powershell
# 10 files (~100k tracks)
for ($i=1; $i -le 10; $i++) { $env:TARGET_TRACKS=10000; python fetch_spotify_10k_min.py }

# 100 files (~1M tracks)
for ($i=1; $i -le 100; $i++) { $env:TARGET_TRACKS=10000; python fetch_spotify_10k_min.py }
```

The script keeps a persistent `seen_track_ids.jsonl`. As long as you don't delete it, each run produces new unique tracks.

## Output columns (CSV)

- `spotify_id` - Unique Spotify track identifier
- `name` - Track title
- `artists` - Artist names (semicolon-separated)
- `album` - Album name
- `genres` - Artist genres (semicolon-separated)
- `popularity` - Spotify popularity score (0-100)
- `duration_ms` - Track duration in milliseconds
- `release_date` - Album release date
- `preview_url` - 30-second preview URL
- `track_url` - Full track URL on Spotify
- `explicit` - Whether track is explicit
- `album_image_url` - Album cover image URL
- `isrc` - International Standard Recording Code

## Tips

- If a run yields few or no tracks, just run again; the script randomizes queries/markets/years each time.
- Keep `.env` private. Share `.env.example` instead.
- Add large outputs to `.gitignore` (see below).

## (Optional) Merge all CSVs to one Parquet

```python
python - << 'PY'
import pandas as pd, glob
import pyarrow as pa, pyarrow.parquet as pq
files = sorted(glob.glob("exports/tracks_*.csv"))
df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)
df = df.drop_duplicates(subset=["spotify_id"])
pq.write_table(pa.Table.from_pandas(df), "all_tracks.parquet")
print("Wrote all_tracks.parquet with", len(df), "rows")
PY
```

## License & safety

- Do not publish your secrets.
- Respect Spotify's terms.
- This is for learning and personal projects.

## File Structure

```
data-pipeline/
├── fetch_spotify_10k_min.py    # Main collection script
├── exports/                    # Generated CSV files
├── seen_track_ids.jsonl        # Track ID deduplication
├── artist_genres_cache.json    # Genre cache
├── .env                        # Your API credentials (private)
└── .env.example               # Example env file (safe to commit)
```

## Troubleshooting

**Rate Limiting**: The script includes automatic retry logic with exponential backoff for rate limits.

**Authentication Errors**: Ensure your Spotify API credentials are correct in `.env`.

**No Tracks Collected**: Try running again - the script uses randomized queries and may need multiple attempts.

**Memory Issues**: For very large datasets, consider processing files in smaller batches.

## Integration with Music Discovery App

The CSV files generated by this script are automatically processed by the main application's data pipeline:

1. **Collection**: Use this script to generate CSV files
2. **Processing**: Run `python create_fast_csv_database.py` to combine and optimize
3. **Integration**: The optimized data is used by the Music Discovery App

This creates a complete pipeline from raw data collection to a fully functional music discovery application.
